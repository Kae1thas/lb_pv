# Лабораторная работа №1: Параллельное умножение матрицы на вектор с использованием MPI в Python

## Цель работы
Изучение базовых принципов программирования в модели передачи сообщений (Message Passing) с использованием библиотеки `mpi4py` для Python. Цель — реализовать параллельное умножение матрицы на вектор на системах с распределённой памятью, сравнить производительность последовательной и параллельной версий и проанализировать результаты.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`
- **Реализация MPI**: OpenMPI
- **Операционная система**: WSL2 на Windows

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры матрицы \( M \) и \( N \) (например, `1000 1000`).
  - `AData.dat`: Матрица \( A \) размером \( M \times N \).
  - `xData.dat`: Вектор \( x \) длиной \( N \).

## Этапы реализации

### Этап 0: Генерация тестовых данных
- Реализована в файле `generate.py`.
- Создаёт файлы `in.dat`, `AData.dat`, `xData.dat` с заданными размерами (M = 1000, N = 1000).
- Использует `numpy.random.rand` для генерации матрицы A и вектора x.

### Этап 1: Последовательная версия
- Реализована в файле `sequential.py`.
- Выполняет умножение матрицы на вектор с использованием `numpy.dot()` для верификации.
- Результаты сохраняются в `Results_seq.dat`.
  
<img width="771" height="47" alt="image" src="https://github.com/user-attachments/assets/4607aba8-581a-4507-97fd-b25f68e18b38" />

### Этап 2: Базовая параллельная версия
- Реализована в файле `parallel_send_recv.py`.
- Использует операции `MPI.Send` и `MPI.Recv` для распределения данных и сбора результатов.
- Требования:
  - Инициализация MPI и получение ранга процесса и общего числа процессов.
  - Чтение параметров \( M \) и \( N \) из файла `in.dat` процессом 0 с использованием `MPI.Bcast`.
  - Распределение матрицы \( A \) и вектора \( x \) с помощью циклов `Send` и `Recv`.
  - Локальное вычисление \( b_{part} = A_{part} \cdot x \).
  - Сбор результатов в файл `Results_parallel.dat`.

<img width="1195" height="610" alt="image" src="https://github.com/user-attachments/assets/dd303130-f6bb-4154-a7fc-717c611770f3" />

### Этап 3: Оптимизация с коллективными операциями
- Реализована в файле `parallel_scatter_gather.py`.
- Заменены операции `Send/Recv` на `MPI.Scatterv` для распределения матрицы и `MPI.Gatherv` для сбора результатов.
- Гарантирована совместимость с предыдущим этапом.
- Результаты сохраняются в `Results_parallel2.dat`.

<img width="1193" height="555" alt="image" src="https://github.com/user-attachments/assets/915611f3-cacb-441b-95bd-2eddfc591085" />

### Этап 4: Обработка произвольного размера матрицы
- Реализована в файле `parallel_scatter_gather_variable.py`.
- Поддерживает случаи, когда \( M \) не делится на число процессов, с использованием массивов `rcounts` и `displs` в `Scatterv` и `Gatherv`.
- Результаты сохраняются в `Results_parallel_variable.dat`.

<img width="1304" height="562" alt="image" src="https://github.com/user-attachments/assets/c33d28af-92ea-46ea-b0be-8baa064b0780" />

## График ускорения
График ускорения (speedup_plot.png) показывает зависимость ( Speedup ) от числа процессов для трёх реализаций:

- Send/Recv (синяя линия): Максимальное ускорение достигается при 4 процессах (~1.09), но падает при 8 процессах (~0.87) из-за накладных расходов.
- Scatterv/Gatherv (оранжевая линия): Ускорение плавное, максимум ~1.05 при 4 процессах, снижение до ~0.95 при 8 процессах.
- Variable M (зелёная линия): Ускорение стабильно около 1.00–1.04, с минимальным спадом до ~0.99 при 8 процессах.
- Анализ: Ускорение неидеальное из-за высоких накладных расходов на коммуникацию в MPI, небольшой размер матрицы (1000×1000), и возможной многопоточности в numpy.dot() в последовательной версии. При 8 процессах наблюдается спад из-за конкуренции за ресурсы и синхронизации.

<img width="640" height="480" alt="speedup_plot" src="https://github.com/user-attachments/assets/e3f78d1d-43b9-4179-9ade-8cf107c3e408" />

## Почему ускорение не идеальное?
- Высокие накладные расходы на коммуникацию в MPI: Передача данных между процессами (особенно в реализациях Send/Recv и Scatterv/Gatherv) требует времени, что снижает эффективность параллелизации.
- Небольшой размер матрицы (1000×1000): Для малых матриц вычислительные задачи недостаточно велики, чтобы компенсировать накладные расходы на коммуникацию и синхронизацию, что ограничивает ускорение.
- Многопоточность в numpy.dot(): Последовательная версия использует оптимизированную функцию numpy.dot(), которая может задействовать многопоточность на уровне процессора, уменьшая относительное преимущество параллельных реализаций.
- Конкуренция за ресурсы при 8 процессах: Увеличение числа процессов (до 8) приводит к спаду ускорения из-за конкуренции за вычислительные ресурсы (например, кэш-память, пропускная способность) и дополнительных затрат на синхронизацию процессов.

## Выводы

Параллельное умножение матрицы на вектор с использованием MPI в Python, реализованное в трёх версиях (Send/Recv, Scatterv/Gatherv, Variable M), демонстрирует ограниченную эффективность для матриц размером 1000×1000 при умеренном числе процессов (2–4). Версия с коллективными операциями Scatterv/Gatherv и поддержкой произвольного размера матрицы (Variable M) показывает более стабильное ускорение по сравнению с базовой реализацией Send/Recv. Однако при увеличении числа процессов до 8 наблюдается спад производительности из-за высоких накладных расходов на коммуникации и конкуренции за ресурсы. Для задач с большими матрицами рекомендуется увеличивать размер задачи, чтобы вычисления доминировали над коммуникациями, и оптимизировать распределение данных для минимизации синхронизационных затрат. Метод MPI эффективен для систем с распределённой памятью, но требует тщательной настройки для достижения оптимальной производительности.
