# Лабораторная работа №1: Параллельное умножение матрицы на вектор с использованием MPI в Python

## Цель работы
Изучение базовых принципов программирования в модели передачи сообщений (Message Passing) с использованием библиотеки `mpi4py` для Python. Цель — реализовать параллельное умножение матрицы на вектор на системах с распределённой памятью, сравнить производительность последовательной и параллельной версий и проанализировать результаты.

### Стек технологий
- **Язык программирования**: Python
- **Библиотеки**: `mpi4py`, `numpy`
- **Реализация MPI**: OpenMPI
- **Операционная система**: WSL2 на Windows

## Этапы реализации

### Этап 1: Последовательная версия
- Реализована в файле `sequential.py`.
- Выполняет умножение матрицы на вектор с использованием `numpy.dot()` для верификации.
- Результаты сохраняются в `Results_seq.dat`.

### Этап 2: Базовая параллельная версия
- Реализована в файле `parallel_send_recv.py`.
- Использует операции `MPI.Send` и `MPI.Recv` для распределения данных и сбора результатов.
- Требования:
  - Инициализация MPI и получение ранга процесса и общего числа процессов.
  - Чтение параметров \( M \) и \( N \) из файла `in.dat` процессом 0 с использованием `MPI.Bcast`.
  - Распределение матрицы \( A \) и вектора \( x \) с помощью циклов `Send` и `Recv`.
  - Локальное вычисление \( b_{part} = A_{part} \cdot x \).
  - Сбор результатов в файл `Results_parallel.dat`.

### Этап 3: Оптимизация с коллективными операциями
- Реализована в файле `parallel_scatter_gather.py`.
- Заменены операции `Send/Recv` на `MPI.Scatterv` для распределения матрицы и `MPI.Gatherv` для сбора результатов.
- Гарантирована совместимость с предыдущим этапом.
- Результаты сохраняются в `Results_parallel2.dat`.

### Этап 4: Обработка произвольного размера матрицы
- Реализована в файле `parallel_scatter_gather_variable.py`.
- Поддерживает случаи, когда \( M \) не делится на число процессов, с использованием массивов `rcounts` и `displs` в `Scatterv` и `Gatherv`.
- Результаты сохраняются в `Results_parallel_variable.dat`.

## Тестовые данные
- **Файлы**:
  - `in.dat`: Содержит размеры матрицы \( M \) и \( N \) (например, `1000 1000`).
  - `AData.dat`: Матрица \( A \) размером \( M \times N \).
  - `xData.dat`: Вектор \( x \) длиной \( N \).
- **Генерация тестовых данных**: Используйте скрипт `generate.py` для создания файлов с заданными размерами.

## График ускорения
График ускорения (speedup_plot.png) показывает зависимость ( Speedup ) от числа процессов для трёх реализаций:

- Send/Recv (синяя линия): Максимальное ускорение достигается при 4 процессах (~1.09), но падает при 8 процессах (~0.87) из-за накладных расходов.
- Scatterv/Gatherv (оранжевая линия): Ускорение плавное, максимум ~1.05 при 4 процессах, снижение до ~0.95 при 8 процессах.
- Variable M (зелёная линия): Ускорение стабильно около 1.00–1.04, с минимальным спадом до ~0.99 при 8 процессах.
- Анализ: Ускорение неидеальное из-за высоких накладных расходов на коммуникацию в MPI, небольшой размер матрицы (1000×1000), и возможной многопоточности в numpy.dot() в последовательной версии. При 8 процессах наблюдается спад из-за конкуренции за ресурсы и синхронизации.
<img width="640" height="480" alt="speedup_plot" src="https://github.com/user-attachments/assets/e3f78d1d-43b9-4179-9ade-8cf107c3e408" />
